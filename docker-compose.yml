# GreekSTT Research Platform - Simplified Docker Compose Configuration
# Focused on ASR models comparison without LLM service

services:
  db:
    image: postgres:16
    container_name: greekstt_db
    restart: unless-stopped
    ports:
      - '5432:5432'
    environment:
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-postgres}
      - POSTGRES_USER=${POSTGRES_USER:-postgres}
      - POSTGRES_DB=${POSTGRES_DB:-greekstt-research}
      - TZ=Europe/Athens
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - greekstt_network
    healthcheck:
      test: ['CMD-SHELL', 'pg_isready -U ${POSTGRES_USER:-postgres}']
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7-alpine
    container_name: greekstt_redis
    restart: unless-stopped
    ports:
      - '6379:6379'
    networks:
      - greekstt_network
    command: redis-server --maxmemory 128mb --maxmemory-policy allkeys-lru --save ""
    healthcheck:
      test: ['CMD', 'redis-cli', 'ping']
      interval: 30s
      timeout: 3s
      retries: 3

  mailhog:
    image: mailhog/mailhog:latest
    container_name: greekstt_mailhog
    restart: unless-stopped
    ports:
      - '1025:1025'
      - '8025:8025'
    networks:
      - greekstt_network

  pgadmin:
    image: dpage/pgadmin4:latest
    container_name: greekstt_pgadmin
    restart: unless-stopped
    ports:
      - '5050:80'
    environment:
      - PGADMIN_DEFAULT_EMAIL=${PGADMIN_EMAIL:-admin@greekstt.research}
      - PGADMIN_DEFAULT_PASSWORD=${PGADMIN_PASSWORD:-admin}
      - PGADMIN_CONFIG_SERVER_MODE=False
      - PGADMIN_CONFIG_MASTER_PASSWORD_REQUIRED=False
    volumes:
      - pgadmin_data:/var/lib/pgadmin
    networks:
      - greekstt_network
    depends_on:
      db:
        condition: service_healthy

  backend:
    build:
      context: .
      dockerfile: Dockerfile.backend
    container_name: greekstt_backend
    restart: unless-stopped
    ports:
      - '5001:5000'
      - '5678:5678'
    environment:
      FLASK_APP: 'run.py'
      FLASK_ENV: 'development'
      FLASK_DEBUG: '1'
      RUNNING_UNDER_DEBUGPY: 'true'
      WHISPER_SERVICE_URL: 'http://whisper-service:8001'
      WAV2VEC2_SERVICE_URL: 'http://wav2vec2-service:8002'
      AI_SERVICE_TIMEOUT: '3600'
      POSTGRES_HOST: 'db'
      POSTGRES_PORT: '5432'
      POSTGRES_DB: ${POSTGRES_DB:-greekstt-research}
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-postgres}
      REDIS_HOST: 'redis'
      REDIS_PORT: '6379'
      REDIS_DB: '0'
      EMAIL_MODE: 'academic_demo'
      SMTP_HOST: 'mailhog'
      SMTP_PORT: '1025'
      FROM_EMAIL: 'noreply@greekstt-research.local'
      FROM_NAME: 'GreekSTT Research Platform'
      PYDEVD_DISABLE_FILE_VALIDATION: '1'
      PYTHONPATH: '/app'
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    volumes:
      - ./apps/backend:/app:delegated
      - backend_uploads:/app/uploads
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/host/root:ro
    networks:
      - greekstt_network
    entrypoint: ['/bin/sh', '-c']
    command: ['python -m debugpy --listen 0.0.0.0:5678 run.py']

  whisper-service:
    build:
      context: ./apps/whisper-service
      dockerfile: Dockerfile
      target: development
    container_name: greekstt_whisper
    restart: unless-stopped
    ports:
      - '8001:8001'
      - '5680:5680'
    environment:
      - PYTHONDONTWRITEBYTECODE=1
      - PYTHONUNBUFFERED=1
      - PYTHONPATH=/app
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=0
      - CUDA_MODULE_LOADING=LAZY
      # GPU Memory allocation settings (60% of RTX 4080)
      - GPU_MEMORY_FRACTION=0.6
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512,expandable_segments:True
      # Model storage for Whisper models
      - HF_HOME=/app/models
      - TRANSFORMERS_CACHE=/app/models/transformers_cache
      - HF_HUB_CACHE=/app/models/hf_hub_cache
      - TORCH_HOME=/app/models/torch_cache
      - WHISPER_CACHE_DIR=/app/models/whisper
      # Optimization settings
      - TRANSFORMERS_OFFLINE=0
      - HF_HUB_ENABLE_HF_TRANSFER=0
      - PYTHONWARNINGS=ignore
      - TRANSFORMERS_VERBOSITY=error
      - TOKENIZERS_PARALLELISM=false
      # CUDA optimizations
      - CUDA_LAUNCH_BLOCKING=0
      - TORCH_CUDA_FOUND_EFFICIENT_ATTENTION=1
      # cuDNN library path fix for faster-whisper
      - LD_LIBRARY_PATH=/usr/local/lib/python3.11/site-packages/ctranslate2.libs:/usr/lib/x86_64-linux-gnu:/usr/local/cuda/lib64
      # Whisper-specific optimizations
      - WHISPER_COMPUTE_TYPE=float16
      - WHISPER_DEVICE=cuda
      - WHISPER_BEAM_SIZE=10
      - WHISPER_BEST_OF=5
      - WHISPER_LANGUAGE=el
      - WHISPER_VAD_FILTER=true
      # Development settings
      - ENVIRONMENT=development
      - DEBUG_MODE=1
    env_file:
      - .env
    volumes:
      - ./apps/whisper-service:/app:delegated
      - whisper_models:/app/models:delegated
      - type: tmpfs
        target: /tmp
        tmpfs:
          size: 8G
    networks:
      - greekstt_network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]
          memory: 16G  # 25% of 64GB RAM allocated to Whisper
        limits:
          memory: 20G  # Maximum memory limit
    shm_size: '2gb'
    ipc: host
    stdin_open: true
    tty: true
    command: python -m debugpy --listen 0.0.0.0:5680 -m uvicorn app.main:app --host 0.0.0.0 --port 8001 --workers 1

  wav2vec2-service:
    build:
      context: ./apps/wav2vec2-service
      dockerfile: Dockerfile
      target: development
    container_name: greekstt_wav2vec2
    restart: unless-stopped
    ports:
      - '8002:8002'
      - '5681:5681'
    environment:
      - PYTHONDONTWRITEBYTECODE=1
      - PYTHONUNBUFFERED=1
      - PYTHONPATH=/app
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=0
      - CUDA_MODULE_LOADING=LAZY
      # GPU Memory allocation settings (40% of RTX 4080)
      - GPU_MEMORY_FRACTION=0.4
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512,expandable_segments:True
      # Model storage for wav2vec2 models
      - HF_HOME=/app/models
      - TRANSFORMERS_CACHE=/app/models/transformers_cache
      - HF_HUB_CACHE=/app/models/hf_hub_cache
      - TORCH_HOME=/app/models/torch_cache
      - WAV2VEC2_CACHE_DIR=/app/models/wav2vec2
      # Optimization settings
      - TRANSFORMERS_OFFLINE=0
      - HF_HUB_ENABLE_HF_TRANSFER=0
      - PYTHONWARNINGS=ignore
      - TRANSFORMERS_VERBOSITY=error
      - TOKENIZERS_PARALLELISM=false
      # CUDA optimizations
      - CUDA_LAUNCH_BLOCKING=0
      - TORCH_CUDA_FOUND_EFFICIENT_ATTENTION=1
      # wav2vec2-specific optimizations
      - WAV2VEC2_MODEL_NAME=lighteternal/wav2vec2-large-xlsr-53-greek
      - WAV2VEC2_CHUNK_LENGTH_S=30
      - WAV2VEC2_STRIDE_LENGTH_S=2
      - WAV2VEC2_DEVICE=cuda
      - WAV2VEC2_SAMPLE_RATE=16000
      # Development settings
      - ENVIRONMENT=development
      - DEBUG_MODE=1
    env_file:
      - .env
    volumes:
      - ./apps/wav2vec2-service:/app:delegated
      - wav2vec2_models:/app/models:delegated
      - type: tmpfs
        target: /tmp
        tmpfs:
          size: 8G
    networks:
      - greekstt_network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]
          memory: 8G   # 12.5% of 64GB RAM allocated to wav2vec2
        limits:
          memory: 12G  # Maximum memory limit
    shm_size: '2gb'
    ipc: host
    stdin_open: true
    tty: true
    command: python -m debugpy --listen 0.0.0.0:5681 -m uvicorn app.main:app --host 0.0.0.0 --port 8002 --workers 1

volumes:
  postgres_data:
  redis_data:
  pgadmin_data:
  backend_uploads:
  whisper_models:
    driver: local
    name: greekstt_whisper_models
  wav2vec2_models:
    driver: local
    name: greekstt_wav2vec2_models

networks:
  greekstt_network:
    driver: bridge
